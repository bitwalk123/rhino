import os

import torch
from torch import optim

from funcs.ios import get_excel_sheet
from funcs.models import get_trained_ppo_model_path
from modules.agent_mask import PolicyNetwork, ValueNetwork, compute_ppo_loss, select_action
from modules.env_mask import TradingEnv
from structs.res import AppRes

if __name__ == "__main__":
    res = AppRes()
    file_excel = "ticks_20250819.xlsx"
    code = "7011"
    path_excel = os.path.join(res.dir_collection, file_excel)
    df = get_excel_sheet(path_excel, code)
    env = TradingEnv(df)

    obs_dim = env.observation_space.shape[0]
    act_dim = env.action_space.n

    """
    ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®åˆæœŸåŒ–
    """
    # è¡Œå‹•åˆ†å¸ƒã‚’å‡ºåŠ›ã™ã‚‹æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    policy_net = PolicyNetwork(obs_dim, act_dim)
    # çŠ¶æ…‹ä¾¡å€¤ã‚’æ¨å®šã™ã‚‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    # ValueNetwork ã¯ å­¦ç¿’æ™‚ã®Advantageè¨ˆç®—å°‚ç”¨
    value_net = ValueNetwork(obs_dim)
    # ä¸¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŒæ™‚ã«æ›´æ–°
    optimizer = optim.Adam(list(policy_net.parameters()) + list(value_net.parameters()), lr=3e-4)

    num_epochs = 3
    gamma = 0.99

    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆPPOï¼‰
    for epoch in range(num_epochs):
        obs_list, action_list, logprob_list, reward_list, mask_list = [], [], [], [], []

        # åˆæœŸçŠ¶æ…‹ã¨ãƒã‚¹ã‚¯å–å¾—
        obs, info = env.reset()
        done = False

        while not done:
            # ç’°å¢ƒã‹ã‚‰å¾—ã‚‰ã‚ŒãŸè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ obs ã‚’ PyTorch ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
            obs_tensor = torch.tensor(obs, dtype=torch.float32)
            # è¡Œå‹•ãƒã‚¹ã‚¯ï¼ˆaction_maskï¼‰ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
            mask_tensor = torch.tensor(info["action_mask"], dtype=torch.float32)
            # ãƒã‚¹ã‚¯ä»˜ãã§è¡Œå‹•åˆ†å¸ƒã‚’ç”Ÿæˆã—ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€log_prob ã¯PPOã®æå¤±è¨ˆç®—ã«å¿…è¦
            action, log_prob = select_action(policy_net, obs_tensor, mask_tensor)

            """
            1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†ã®å±¥æ­´ã‚’å¾Œã§ãƒãƒƒãƒåŒ–ã—ã¦ã€PPOã®æå¤±é–¢æ•°ã«æ¸¡ã™
            ã“ã®ã‚ˆã†ã«ã™ã‚‹ã¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å…¨ä½“ã‚’1ã¤ã®ãƒ†ãƒ³ã‚½ãƒ«ãƒãƒƒãƒã¨ã—ã¦æ‰±ãˆã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
            """
            # ç¾åœ¨ã®è¦³æ¸¬ï¼ˆçŠ¶æ…‹ï¼‰ã‚’ä¿å­˜
            # å¾Œã§ torch.stack(obs_list) ã«ã‚ˆã£ã¦ãƒãƒƒãƒåŒ–ã•ã‚Œã€æ–¹ç­–ãƒ»ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¥åŠ›ã«ãªã‚‹
            obs_list.append(obs_tensor)
            # é¸æŠã—ãŸè¡Œå‹•ã‚’ä¿å­˜
            # torch.tensor(action) ã«ã‚ˆã£ã¦ãƒ†ãƒ³ã‚½ãƒ«åŒ–ã—ã€å¾Œã§æå¤±è¨ˆç®—ã«ä½¿ç”¨
            action_list.append(torch.tensor(action))
            # é¸æŠã—ãŸè¡Œå‹•ã®å¯¾æ•°ç¢ºç‡ã‚’ä¿å­˜
            # log_prob ã¯ Categorical(...).log_prob(action) ã§å¾—ã‚‰ã‚ŒãŸå€¤
            logprob_list.append(log_prob)
            # è¡Œå‹•ãƒã‚¹ã‚¯ã‚’ä¿å­˜
            # PPOã®æå¤±è¨ˆç®—ã§ã€ãƒã‚¹ã‚¯ã•ã‚ŒãŸè¡Œå‹•ã‚’é™¤å¤–ã™ã‚‹ãŸã‚ã«ä½¿ç”¨
            mask_list.append(mask_tensor)

            # çŠ¶æ…‹é·ç§»ã¨å ±é…¬å–å¾—
            obs, reward, done, _, info = env.step(action)
            reward_list.append(torch.tensor(reward, dtype=torch.float32))

        """
        PPOï¼ˆProximal Policy Optimizationï¼‰ã«ãŠã‘ã‚‹ã€Œå‰²å¼•å ±é…¬ï¼ˆReturnï¼‰ã€ã®è¨ˆç®—å‡¦ç†
        ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å…¨ä½“ã®å ±é…¬å±¥æ­´ã‹ã‚‰ã€å„æ™‚ç‚¹ã§ã®ç´¯ç©å ±é…¬ï¼ˆReturnï¼‰ã‚’é€†é †ã§è¨ˆç®—ã€‚
        
        PPOã§ã¯ã€çŠ¶æ…‹ã®ä¾¡å€¤ï¼ˆvalueï¼‰ã¨å®Ÿéš›ã®Returnã¨ã®å·®åˆ†ï¼ˆAdvantageï¼‰ã‚’ä½¿ã£ã¦å­¦ç¿’
        ğ´_ğ‘¡ = ğº_ğ‘¡ âˆ’ ğ‘‰(ğ‘ _ğ‘¡)
        ãã®ãŸã‚ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã®Return ğº_ğ‘¡ ã‚’æ­£ç¢ºã«è¨ˆç®—ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚‹
        """
        # å¼·åŒ–å­¦ç¿’ã§ã¯ã€ã‚ã‚‹æ™‚ç‚¹ ğ‘¡ ã«ãŠã‘ã‚‹ã€ŒReturnã€ğº_ğ‘¡ ã¯ã€
        # ãã®æ™‚ç‚¹ã‹ã‚‰å°†æ¥ã«ã‚ãŸã£ã¦å¾—ã‚‰ã‚Œã‚‹å ±é…¬ã®åˆè¨ˆ
        returns = []
        G = 0
        # å ±é…¬ãƒªã‚¹ãƒˆã‚’å¾Œã‚ã‹ã‚‰å‰ã¸å‡¦ç†
        for r in reversed(reward_list):
            # ç¾åœ¨ã®å ±é…¬ ğ‘Ÿ ã«ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ç´¯ç©å ±é…¬ ğº ã‚’å‰²å¼•ã—ã¦åŠ ãˆã‚‹
            # ã“ã‚Œã«ã‚ˆã‚Šã€æœªæ¥ã®å ±é…¬ã‚’è€ƒæ…®ã—ãŸç´¯ç©å€¤ãŒå¾—ã‚‰ã‚Œã‚‹
            G = r + gamma * G
            # returns ã®å…ˆé ­ã« ğº ã‚’æŒ¿å…¥ã™ã‚‹ã“ã¨ã§ã€å…ƒã®æ™‚é–“é †ã«æˆ»ã™
            returns.insert(0, G)

        """
        PPOã®æå¤±è¨ˆç®—ã«å‘ã‘ãŸã€ŒãƒãƒƒãƒåŒ–ã¨å‰å‡¦ç†ã€
        """
        # æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ»ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¥åŠ›
        obs_batch = torch.stack(obs_list)
        # å„ã‚¹ãƒ†ãƒƒãƒ—ã§é¸æŠã—ãŸè¡Œå‹•ï¼ˆæ•´æ•°ï¼‰ã‚’ãƒ†ãƒ³ã‚½ãƒ«åŒ–
        # PPOã®æå¤±é–¢æ•°ã§ã¯ã€è¡Œå‹•ã”ã¨ã®ç¢ºç‡ã‚„ä¾¡å€¤ã‚’æ¯”è¼ƒã™ã‚‹ãŸã‚ã«å¿…è¦
        action_batch = torch.stack(action_list)
        # å„è¡Œå‹•ã®å¯¾æ•°ç¢ºç‡ï¼ˆlog_probï¼‰ã‚’ã¾ã¨ã‚ã‚‹
        # PPOã§ã¯ã€Œæ–°ã—ã„æ–¹ç­–ã¨å¤ã„æ–¹ç­–ã®ç¢ºç‡æ¯”ã€ã‚’ä½¿ã£ã¦æå¤±ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã€
        # éå»ã®ç¢ºç‡ã‚’è¨˜éŒ²ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚‹
        logprob_batch = torch.stack(logprob_list)
        # å„ã‚¹ãƒ†ãƒƒãƒ—ã®å‰²å¼•å ±é…¬ï¼ˆReturnï¼‰ã‚’ã¾ã¨ã‚ã‚‹
        # ã“ã‚Œã¯ã€Œå®Ÿéš›ã«å¾—ã‚‰ã‚ŒãŸå ±é…¬ã®ç´¯ç©ã€ã§ã‚ã‚Šã€
        # ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã®æ¯”è¼ƒã«ä½¿ã†
        return_batch = torch.stack(returns)
        # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ« obs_batch ã‚’ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é€šã—ã¦ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã®çŠ¶æ…‹ä¾¡å€¤ ğ‘‰(ğ‘ _ğ‘¡) ã‚’å–å¾—
        # .squeeze() ã«ã‚ˆã£ã¦ [T, 1] â†’ [T] ã«å¤‰å½¢ï¼ˆæå¤±è¨ˆç®—ã®ãŸã‚ï¼‰
        #value_batch = value_net(obs_batch).squeeze()
        value_batch = value_net(obs_batch).squeeze(-1)  # æœ€å¾Œã®æ¬¡å…ƒã ã‘ã‚’æ½°ã™
        # Advantageï¼ˆåˆ©å¾—ï¼‰ã®è¨ˆç®—
        # ğ´_ğ‘¡ = ğº_ğ‘¡ âˆ’ ğ‘‰(ğ‘ _ğ‘¡)
        # detach() ã«ã‚ˆã£ã¦ã€ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‹¾é…è¨ˆç®—ã‚’åˆ‡ã‚Šé›¢ã™ã€‚
        # ï¼ˆæ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã ã‘ã‚’æ›´æ–°ã™ã‚‹ãŸã‚ï¼‰
        adv_batch = return_batch - value_batch.detach()
        # å„ã‚¹ãƒ†ãƒƒãƒ—ã®è¡Œå‹•ãƒã‚¹ã‚¯ã‚’ã¾ã¨ã‚ã‚‹
        # PPOã®æå¤±è¨ˆç®—ã§ã€é¸æŠä¸å¯èƒ½ãªè¡Œå‹•ã‚’é™¤å¤–ã™ã‚‹ãŸã‚ã«ä½¿ã†
        mask_batch = torch.stack(mask_list)
        """
        PPOã®æå¤±é–¢æ•°ã‚’è¨ˆç®—ï¼ˆæ–¹ç­–ãƒ»ä¾¡å€¤ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …ã‚’å«ã‚€ï¼‰
        """
        loss = compute_ppo_loss(
            policy_net,
            value_net,
            obs_batch,
            action_batch,
            logprob_batch,
            return_batch,
            adv_batch,
            mask_batch
        )
        # å‹¾é…ã‚’ã‚¼ãƒ­ã«åˆæœŸåŒ–
        # å‰å›ã®å‹¾é…æƒ…å ±ã‚’æ¶ˆå»ã—ã¦ã€ä»Šå›ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã«å‚™ãˆã‚‹ãŸã‚ã®å¿…é ˆæ“ä½œ
        optimizer.zero_grad()
        # æå¤±é–¢æ•°ã‹ã‚‰å‹¾é…ã‚’è¨ˆç®—
        loss.backward()
        # å‹¾é…ã«åŸºã¥ã„ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
        optimizer.step()

        print(f"Epoch {epoch + 1}: Loss = {loss.item():.4f}")

    # å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    # https://docs.pytorch.org/docs/stable/generated/torch.save.html
    model_path = get_trained_ppo_model_path(res, code)
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    obj = {
        "policy_state_dict": policy_net.state_dict(),
        "value_state_dict": value_net.state_dict()
    }
    torch.save(obj, model_path)
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_path}")
