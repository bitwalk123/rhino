import pandas as pd
import torch
import torch.nn as nn
from torch import optim
from torch.distributions import Categorical

from modules.env_mask import TradingEnv


def compute_ppo_loss(
        policy_net,
        value_net,
        obs, actions,
        old_log_probs,
        returns,
        advantages,
        action_masks
):
    """
    ğŸ“¦ PPOæå¤±é–¢æ•°ï¼ˆClipä»˜ãï¼‰
    ã“ã®é–¢æ•°ã¯ã€ä»¥ä¸‹ã®2ã¤ã®æå¤±ã‚’è¨ˆç®—ã—ã¦åˆæˆã—ã¾ã™ï¼š
    - æ–¹ç­–æå¤±ï¼ˆpolicy_lossï¼‰ï¼šç¢ºç‡æ¯”ç‡ã®ã‚¯ãƒªãƒƒãƒ—ä»˜ãæå¤±
    - ä¾¡å€¤æå¤±ï¼ˆvalue_lossï¼‰ï¼šçŠ¶æ…‹ä¾¡å€¤ã¨Returnã®èª¤å·®ï¼ˆMSEï¼‰
    """
    # æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«çŠ¶æ…‹ã¨ãƒã‚¹ã‚¯ã‚’æ¸¡ã—ã¦ã€å„è¡Œå‹•ã®ãƒ­ã‚¸ãƒƒãƒˆï¼ˆæœªæ­£è¦åŒ–ã‚¹ã‚³ã‚¢ï¼‰ã‚’å–å¾—
    logits = policy_net(obs, action_masks)
    # ãƒ­ã‚¸ãƒƒãƒˆã‹ã‚‰ ç¢ºç‡åˆ†å¸ƒï¼ˆCategoricalï¼‰ ã‚’æ§‹ç¯‰
    dist = Categorical(logits=logits)
    # ç¾åœ¨ã®æ–¹ç­–ã§ã€éå»ã«é¸æŠã•ã‚ŒãŸè¡Œå‹•ã®å¯¾æ•°ç¢ºç‡ã‚’å–å¾—
    new_log_probs = dist.log_prob(actions)

    # PPOã®ä¸­æ ¸ï¼šç¢ºç‡æ¯”ç‡ï¼ˆæ–°æ—§æ–¹ç­–ã®ç¢ºç‡ã®å¤‰åŒ–ç‡ï¼‰ã‚’è¨ˆç®—
    ratio = torch.exp(new_log_probs - old_log_probs)
    # PPOã®ã€ŒClipä»˜ãæå¤±ã€ã®ãŸã‚ã«ã€ç¢ºç‡æ¯”ç‡ã‚’ä¸Šä¸‹ã«åˆ¶é™
    # 1 Â± 0.2 ã¯ ã‚¯ãƒªãƒƒãƒ—ç¯„å›²ï¼ˆÎµï¼‰ï¼šã“ã®ç¯„å›²ã‚’è¶…ãˆã‚‹æ›´æ–°ã¯æŠ‘åˆ¶ã•ã‚Œã‚‹
    # ã“ã‚Œã«ã‚ˆã‚Šã€æ–¹ç­–ã®æ€¥æ¿€ãªå¤‰åŒ–ã‚’é˜²ãã€å®‰å®šã—ãŸå­¦ç¿’ãŒå¯èƒ½ã«ãªã‚‹
    clipped_ratio = torch.clamp(ratio, 1 - 0.2, 1 + 0.2)
    # PPOã®ã€ŒSurrogate Objectiveã€
    # advantages ã¯ [T] ã®ãƒ†ãƒ³ã‚½ãƒ«ã§ã€è¡Œå‹•ã®è‰¯ã•ã‚’è¡¨ã™é‡ã¿
    # torch.min(...) ã«ã‚ˆã£ã¦ã€ã‚¯ãƒªãƒƒãƒ—ã•ã‚ŒãŸæ–¹ç­–æå¤±ãŒé¸ã°ã‚Œã‚‹
    # - ã‚’ä»˜ã‘ã‚‹ã“ã¨ã§ã€æå¤±é–¢æ•°ã¨ã—ã¦æœ€å°åŒ–å¯¾è±¡ã«ã™ã‚‹
    policy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()

    # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ« obs ã‚’ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é€šã—ã¦ã€çŠ¶æ…‹ä¾¡å€¤ ğ‘‰(ğ‘ _ğ‘¡) ã‚’å–å¾—
    # .squeeze() ã«ã‚ˆã£ã¦ [T, 1] â†’ [T] ã«å¤‰å½¢ï¼ˆæå¤±è¨ˆç®—ã®ãŸã‚ï¼‰
    values = value_net(obs).squeeze()
    # çŠ¶æ…‹ä¾¡å€¤ã¨å®Ÿéš›ã® Return ã®èª¤å·®ã‚’ MSEï¼ˆå¹³å‡äºŒä¹—èª¤å·®ï¼‰ã§è¨ˆç®—
    value_loss = nn.functional.mse_loss(values, returns)
    # æœ€çµ‚çš„ãªæå¤±ã¯ã€æ–¹ç­–æå¤± + ä¾¡å€¤æå¤±ï¼ˆé‡ã¿ä»˜ãï¼‰
    # 0.5 ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã€ä¾¡å€¤æå¤±ã®å½±éŸ¿åº¦ã‚’èª¿æ•´
    # ã“ã®åˆæˆæå¤±ã‚’ loss.backward() ã«æ¸¡ã—ã¦ã€ä¸¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’åŒæ™‚ã«æ›´æ–°
    return policy_loss + 0.5 * value_loss


def select_action(policy_net, obs, action_mask):
    """
    ğŸš¦ è¡Œå‹•é¸æŠé–¢æ•°
    ç¾åœ¨ã®çŠ¶æ…‹ obs ã¨è¡Œå‹•ãƒã‚¹ã‚¯ action_mask ã‚’ä½¿ã£ã¦ã€æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã‚‰è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹
    PPOã§ã¯ã€ç¢ºç‡çš„æ–¹ç­–ã«åŸºã¥ã„ã¦è¡Œå‹•ã‚’é¸ã³ã€ãã®ç¢ºç‡ï¼ˆlog_probï¼‰ã‚‚è¨˜éŒ²ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
    """
    # æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«çŠ¶æ…‹ã¨ãƒã‚¹ã‚¯ã‚’æ¸¡ã—ã¦ã€è¡Œå‹•ã®ãƒ­ã‚¸ãƒƒãƒˆï¼ˆæœªæ­£è¦åŒ–ã‚¹ã‚³ã‚¢ï¼‰ã‚’å–å¾—
    logits = policy_net(obs, action_mask)
    # Categorical åˆ†å¸ƒã‚’ä½¿ã£ã¦ã€ãƒ­ã‚¸ãƒƒãƒˆã‹ã‚‰ç¢ºç‡åˆ†å¸ƒã‚’æ§‹ç¯‰
    dist = Categorical(logits=logits)
    # åˆ†å¸ƒ dist ã‹ã‚‰ ç¢ºç‡çš„ã«è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    action = dist.sample()
    # é¸æŠã—ãŸè¡Œå‹•ã®å¯¾æ•°ç¢ºç‡ï¼ˆlog_probï¼‰ã‚’å–å¾—
    log_prob = dist.log_prob(action)
    # action.item() ã«ã‚ˆã£ã¦ã€ãƒ†ãƒ³ã‚½ãƒ«ã‹ã‚‰Pythonã®æ•´æ•°ã«å¤‰æ›
    # log_prob ã¯ãã®ã¾ã¾ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã™ï¼ˆå¾Œã§ loss.backward() ã«ä½¿ã†ãŸã‚ï¼‰
    return action.item(), log_prob


class PolicyNetwork(nn.Module):
    # ğŸ¯ æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆãƒã‚¹ã‚¯å¯¾å¿œï¼‰
    def __init__(self, obs_dim: int, act_dim: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, act_dim)
        )

    def forward(self, obs, action_mask=None):
        logits = self.net(obs)
        if action_mask is not None:
            logits = logits.masked_fill(action_mask == 0, float('-inf'))
        return logits


class ValueNetwork(nn.Module):
    # ğŸ§  ValueNetwork ã®åŸºæœ¬æ§‹é€ 
    def __init__(self, obs_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)  # å‡ºåŠ›ã¯ã‚¹ã‚«ãƒ©ãƒ¼ï¼ˆçŠ¶æ…‹ä¾¡å€¤ï¼‰
        )

    def forward(self, obs):
        return self.net(obs)  # shape: [batch_size, 1]


class PPOAgent:
    def __init__(self):
        pass

    def infer(self, df: pd.DataFrame, model_path: str):
        env = TradingEnv(df)
        obs_dim = env.observation_space.shape[0]
        act_dim = env.action_space.n

        # ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿
        checkpoint = torch.load(model_path)
        policy_net = PolicyNetwork(obs_dim, act_dim)
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¾©å…ƒ
        policy_net.load_state_dict(checkpoint["policy_state_dict"])
        # .eval() ã«ã‚ˆã£ã¦æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆï¼ˆDropoutã‚„BatchNormãŒç„¡åŠ¹åŒ–ï¼‰
        policy_net.eval()

        # _/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_
        # æ¨è«–ãƒ«ãƒ¼ãƒ—
        # _/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_
        obs, info = env.reset()
        done = False
        while not done:
            # çŠ¶æ…‹ã¨ãƒã‚¹ã‚¯ã‚’ãƒ†ãƒ³ã‚½ãƒ«åŒ–ï¼ˆPyTorchãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«æ¸¡ã™ãŸã‚ï¼‰
            obs_tensor = torch.tensor(obs, dtype=torch.float32)
            mask_tensor = torch.tensor(info["action_mask"], dtype=torch.float32)
            # ãƒã‚¹ã‚¯ä»˜ãã§è¡Œå‹•åˆ†å¸ƒã‚’ç”Ÿæˆã—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€log_prob ã¯æ¨è«–ã§ã¯ä½¿ã‚ãªã„ãŒã€ãƒ­ã‚°ã‚„åˆ†æã«æ´»ç”¨å¯èƒ½
            action, log_prob = select_action(policy_net, obs_tensor, mask_tensor)

            """
            é¸æŠã•ã‚ŒãŸè¡Œå‹•ãŒãƒã‚¹ã‚¯ã§ç¦æ­¢ã•ã‚Œã¦ã„ãªã„ã‹ã‚’ç¢ºèª
            mask_tensor[action] == 0 ã®å ´åˆã¯é•åè¡Œå‹•ï¼ˆè¨­è¨ˆãƒŸã‚¹ã‚„ãƒã‚°ã®æ¤œå‡ºã«æœ‰åŠ¹ï¼‰
            select_action() å†…éƒ¨ã§ãƒã‚¹ã‚¯ãŒæ­£ã—ãé©ç”¨ã•ã‚Œã¦ã„ã‚Œã°ã€é€šå¸¸ã¯é•åã¯èµ·ããªã„
            """
            if mask_tensor[action] == 0:
                print(f"âŒ é•åè¡Œå‹•: {action}, Mask: {mask_tensor.tolist()}")
            """
            else:
                print(f"âœ” è¡Œå‹•: {action}, Mask: {mask_tensor.tolist()}")
            """

            obs, reward, done, _, info = env.step(action)

        # å–å¼•æ˜ç´°ã®å‡ºåŠ›
        df_transaction = pd.DataFrame(env.transman.dict_transaction)
        print(df_transaction)
        print(f"ä¸€æ ªå½“ã‚Šã®æç›Š : {df_transaction['æç›Š'].sum()} å††")

    def train(self, df: pd.DataFrame, model_path: str):
        env = TradingEnv(df)
        obs_dim = env.observation_space.shape[0]
        act_dim = env.action_space.n

        """
        ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®åˆæœŸåŒ–
        """
        # è¡Œå‹•åˆ†å¸ƒã‚’å‡ºåŠ›ã™ã‚‹æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        policy_net = PolicyNetwork(obs_dim, act_dim)
        # çŠ¶æ…‹ä¾¡å€¤ã‚’æ¨å®šã™ã‚‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        # ValueNetwork ã¯ å­¦ç¿’æ™‚ã®Advantageè¨ˆç®—å°‚ç”¨
        value_net = ValueNetwork(obs_dim)
        # ä¸¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŒæ™‚ã«æ›´æ–°
        optimizer = optim.Adam(list(policy_net.parameters()) + list(value_net.parameters()), lr=3e-4)

        num_epochs = 3
        gamma = 0.99

        # _/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_
        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        # _/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_
        for epoch in range(num_epochs):
            obs_list, action_list, logprob_list, reward_list, mask_list = [], [], [], [], []

            # åˆæœŸçŠ¶æ…‹ã¨ãƒã‚¹ã‚¯å–å¾—
            obs, info = env.reset()
            done = False
            while not done:
                # ç’°å¢ƒã‹ã‚‰å¾—ã‚‰ã‚ŒãŸè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ obs ã‚’ PyTorch ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
                obs_tensor = torch.tensor(obs, dtype=torch.float32)
                # è¡Œå‹•ãƒã‚¹ã‚¯ï¼ˆaction_maskï¼‰ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
                mask_tensor = torch.tensor(info["action_mask"], dtype=torch.float32)
                # ãƒã‚¹ã‚¯ä»˜ãã§è¡Œå‹•åˆ†å¸ƒã‚’ç”Ÿæˆã—ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€log_prob ã¯PPOã®æå¤±è¨ˆç®—ã«å¿…è¦
                action, log_prob = select_action(policy_net, obs_tensor, mask_tensor)

                """
                1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†ã®å±¥æ­´ã‚’å¾Œã§ãƒãƒƒãƒåŒ–ã—ã¦ã€PPOã®æå¤±é–¢æ•°ã«æ¸¡ã™
                ã“ã®ã‚ˆã†ã«ã™ã‚‹ã¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å…¨ä½“ã‚’1ã¤ã®ãƒ†ãƒ³ã‚½ãƒ«ãƒãƒƒãƒã¨ã—ã¦æ‰±ãˆã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
                """
                # ç¾åœ¨ã®è¦³æ¸¬ï¼ˆçŠ¶æ…‹ï¼‰ã‚’ä¿å­˜
                obs_list.append(obs_tensor)
                # é¸æŠã—ãŸè¡Œå‹•ã‚’ä¿å­˜
                action_list.append(torch.tensor(action))
                # é¸æŠã—ãŸè¡Œå‹•ã®å¯¾æ•°ç¢ºç‡ã‚’ä¿å­˜
                logprob_list.append(log_prob)
                # è¡Œå‹•ãƒã‚¹ã‚¯ã‚’ä¿å­˜
                mask_list.append(mask_tensor)

                # çŠ¶æ…‹é·ç§»ã¨å ±é…¬å–å¾—
                obs, reward, done, _, info = env.step(action)
                reward_list.append(torch.tensor(reward, dtype=torch.float32))

            """
            PPOï¼ˆProximal Policy Optimizationï¼‰ã«ãŠã‘ã‚‹ã€Œå‰²å¼•å ±é…¬ï¼ˆReturnï¼‰ã€ã®è¨ˆç®—å‡¦ç†
            ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å…¨ä½“ã®å ±é…¬å±¥æ­´ã‹ã‚‰ã€å„æ™‚ç‚¹ã§ã®ç´¯ç©å ±é…¬ï¼ˆReturnï¼‰ã‚’é€†é †ã§è¨ˆç®—ã€‚

            PPOã§ã¯ã€çŠ¶æ…‹ã®ä¾¡å€¤ï¼ˆvalueï¼‰ã¨å®Ÿéš›ã®Returnã¨ã®å·®åˆ†ï¼ˆAdvantageï¼‰ã‚’ä½¿ã£ã¦å­¦ç¿’
            ğ´_ğ‘¡ = ğº_ğ‘¡ âˆ’ ğ‘‰(ğ‘ _ğ‘¡)
            ãã®ãŸã‚ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã®Return ğº_ğ‘¡ ã‚’æ­£ç¢ºã«è¨ˆç®—ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚‹
            """
            # ã‚ã‚‹æ™‚ç‚¹ ğ‘¡ ã«ãŠã‘ã‚‹ã€ŒReturnã€ğº_ğ‘¡ ã¯ã€ãã®æ™‚ç‚¹ã‹ã‚‰å°†æ¥ã«ã‚ãŸã£ã¦å¾—ã‚‰ã‚Œã‚‹å ±é…¬ã®åˆè¨ˆ
            returns = []
            G = 0
            # å ±é…¬ãƒªã‚¹ãƒˆã‚’å¾Œã‚ã‹ã‚‰å‰ã¸å‡¦ç†
            for r in reversed(reward_list):
                # ç¾åœ¨ã®å ±é…¬ ğ‘Ÿ ã«ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ç´¯ç©å ±é…¬ ğº ã‚’å‰²å¼•ã—ã¦åŠ ãˆã‚‹
                # ã“ã‚Œã«ã‚ˆã‚Šã€æœªæ¥ã®å ±é…¬ã‚’è€ƒæ…®ã—ãŸç´¯ç©å€¤ãŒå¾—ã‚‰ã‚Œã‚‹
                G = r + gamma * G
                # returns ã®å…ˆé ­ã« ğº ã‚’æŒ¿å…¥ã™ã‚‹ã“ã¨ã§ã€å…ƒã®æ™‚é–“é †ã«æˆ»ã™
                returns.insert(0, G)

            """
            PPOã®æå¤±è¨ˆç®—ã«å‘ã‘ãŸã€ŒãƒãƒƒãƒåŒ–ã¨å‰å‡¦ç†ã€
            """
            # æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ»ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¥åŠ›
            obs_batch = torch.stack(obs_list)
            # å„ã‚¹ãƒ†ãƒƒãƒ—ã§é¸æŠã—ãŸè¡Œå‹•ï¼ˆæ•´æ•°ï¼‰ã‚’ãƒ†ãƒ³ã‚½ãƒ«åŒ–
            action_batch = torch.stack(action_list)
            # å„è¡Œå‹•ã®å¯¾æ•°ç¢ºç‡ï¼ˆlog_probï¼‰ã‚’ã¾ã¨ã‚ã‚‹
            logprob_batch = torch.stack(logprob_list)
            # å„ã‚¹ãƒ†ãƒƒãƒ—ã®å‰²å¼•å ±é…¬ï¼ˆReturnï¼‰ã‚’ã¾ã¨ã‚ã‚‹
            return_batch = torch.stack(returns)
            # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ« obs_batch ã‚’ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é€šã—ã¦ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã®çŠ¶æ…‹ä¾¡å€¤ ğ‘‰(ğ‘ _ğ‘¡) ã‚’å–å¾—
            value_batch = value_net(obs_batch).squeeze(-1)  # æœ€å¾Œã®æ¬¡å…ƒã ã‘ã‚’æ½°ã™
            # Advantageï¼ˆåˆ©å¾—ï¼‰ã®è¨ˆç®—
            adv_batch = return_batch - value_batch.detach()
            # å„ã‚¹ãƒ†ãƒƒãƒ—ã®è¡Œå‹•ãƒã‚¹ã‚¯ã‚’ã¾ã¨ã‚ã‚‹
            mask_batch = torch.stack(mask_list)

            """
            PPOã®æå¤±é–¢æ•°ã‚’è¨ˆç®—ï¼ˆæ–¹ç­–ãƒ»ä¾¡å€¤ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …ã‚’å«ã‚€ï¼‰
            """
            loss = compute_ppo_loss(
                policy_net,
                value_net,
                obs_batch,
                action_batch,
                logprob_batch,
                return_batch,
                adv_batch,
                mask_batch
            )
            # å‹¾é…ã‚’ã‚¼ãƒ­ã«åˆæœŸåŒ–
            optimizer.zero_grad()
            # æå¤±é–¢æ•°ã‹ã‚‰å‹¾é…ã‚’è¨ˆç®—
            loss.backward()
            # å‹¾é…ã«åŸºã¥ã„ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
            optimizer.step()

            print(f"Epoch {epoch + 1}: Loss = {loss.item():.4f}")

        # å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        # https://docs.pytorch.org/docs/stable/generated/torch.save.html
        obj = {
            "policy_state_dict": policy_net.state_dict(),
            "value_state_dict": value_net.state_dict()
        }
        torch.save(obj, model_path)
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_path}")
