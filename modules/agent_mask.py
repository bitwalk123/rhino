import os

import pandas as pd
import torch
import torch.nn as nn
from torch import optim, Tensor
from torch.distributions import Categorical

from modules.env_mask import TrainingEnv


class PolicyNetwork(nn.Module):
    # ğŸ¯ æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆãƒã‚¹ã‚¯å¯¾å¿œï¼‰
    def __init__(self, obs_dim: int, act_dim: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, act_dim)
        )

    def forward(self, obs, action_mask=None):
        logits = self.net(obs)
        if action_mask is not None:
            logits = logits.masked_fill(action_mask == 0, float('-inf'))
        return logits


class ValueNetwork(nn.Module):
    # ğŸ’° ValueNetwork ã®åŸºæœ¬æ§‹é€ 
    def __init__(self, obs_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)  # å‡ºåŠ›ã¯ã‚¹ã‚«ãƒ©ãƒ¼ï¼ˆçŠ¶æ…‹ä¾¡å€¤ï¼‰
        )

    def forward(self, obs):
        return self.net(obs)  # shape: [batch_size, 1]


class PPOAgent:
    def __init__(self):
        self.env = None  # ç’°å¢ƒã¯å­¦ç¿’ã¨æ¨è«–ã§ç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã€ã“ã“ã§ã¯ç©ºã«ã™ã‚‹
        self.policy_net = None  # æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.value_net = None  # çŠ¶æ…‹ä¾¡å€¤ã‚’æ¨å®šã™ã‚‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.optimizer = None  # ä¸¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŒæ™‚ã«æ›´æ–°ã™ã‚‹ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        # ---------------------------------------------------------------------
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        # ---------------------------------------------------------------------
        self.clip_epsilon = 0.2  # PPOã‚¯ãƒªãƒƒãƒ—ä¿‚æ•° (Clip Epsilon (Îµ))
        self.entropy_coef = 0.01  # æ¢ç´¢ä¿ƒé€²ã®ãŸã‚ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …ã®é‡ã¿
        self.gamma = 0.99  # å‰²å¼•ç‡ï¼ˆdiscount factorï¼‰
        self.lr = 3e-4  # å­¦ç¿’ç‡ (Learning Rate)
        self.value_coef = 0.5  # ä¾¡å€¤æå¤±ä¿‚æ•° (Value Loss Coefficient)

    def compute_ppo_loss(
            self,
            obs,
            actions,
            old_log_probs,
            returns,
            advantages,
            action_masks
    ):
        """
        ğŸ“¦ PPOæå¤±é–¢æ•°ï¼ˆClipä»˜ãï¼‰
        ã“ã®é–¢æ•°ã¯ã€ä»¥ä¸‹ã®2ã¤ã®æå¤±ã‚’è¨ˆç®—ã—ã¦åˆæˆã—ã¾ã™ï¼š
        - æ–¹ç­–æå¤±ï¼ˆpolicy_lossï¼‰ï¼šç¢ºç‡æ¯”ç‡ã®ã‚¯ãƒªãƒƒãƒ—ä»˜ãæå¤±
        - ä¾¡å€¤æå¤±ï¼ˆvalue_lossï¼‰ï¼šçŠ¶æ…‹ä¾¡å€¤ã¨Returnã®èª¤å·®ï¼ˆMSEï¼‰
        """
        # æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«çŠ¶æ…‹ã¨ãƒã‚¹ã‚¯ã‚’æ¸¡ã—ã¦ã€å„è¡Œå‹•ã®ãƒ­ã‚¸ãƒƒãƒˆï¼ˆæœªæ­£è¦åŒ–ã‚¹ã‚³ã‚¢ï¼‰ã‚’å–å¾—
        logits = self.policy_net(obs, action_masks)
        # ãƒ­ã‚¸ãƒƒãƒˆã‹ã‚‰ ç¢ºç‡åˆ†å¸ƒï¼ˆCategoricalï¼‰ ã‚’æ§‹ç¯‰
        dist = Categorical(logits=logits)
        # ç¾åœ¨ã®æ–¹ç­–ã§ã€éå»ã«é¸æŠã•ã‚ŒãŸè¡Œå‹•ã®å¯¾æ•°ç¢ºç‡ã‚’å–å¾—
        new_log_probs = dist.log_prob(actions)

        # PPOã®ä¸­æ ¸ï¼šç¢ºç‡æ¯”ç‡ï¼ˆæ–°æ—§æ–¹ç­–ã®ç¢ºç‡ã®å¤‰åŒ–ç‡ï¼‰ã‚’è¨ˆç®—
        ratio = torch.exp(new_log_probs - old_log_probs)
        # PPOã®ã€ŒClipä»˜ãæå¤±ã€ã®ãŸã‚ã«ã€ç¢ºç‡æ¯”ç‡ã‚’ä¸Šä¸‹ã«åˆ¶é™
        # ã“ã‚Œã«ã‚ˆã‚Šã€æ–¹ç­–ã®æ€¥æ¿€ãªå¤‰åŒ–ã‚’é˜²ãã€å®‰å®šã—ãŸå­¦ç¿’ãŒå¯èƒ½ã«ãªã‚‹
        clipped_ratio = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon)
        # PPOã®ã€ŒSurrogate Objectiveã€
        # advantages ã¯ [T] ã®ãƒ†ãƒ³ã‚½ãƒ«ã§ã€è¡Œå‹•ã®è‰¯ã•ã‚’è¡¨ã™é‡ã¿
        # torch.min(...) ã«ã‚ˆã£ã¦ã€ã‚¯ãƒªãƒƒãƒ—ã•ã‚ŒãŸæ–¹ç­–æå¤±ãŒé¸ã°ã‚Œã‚‹
        # - ã‚’ä»˜ã‘ã‚‹ã“ã¨ã§ã€æå¤±é–¢æ•°ã¨ã—ã¦æœ€å°åŒ–å¯¾è±¡ã«ã™ã‚‹
        policy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()

        # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ« obs ã‚’ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é€šã—ã¦ã€çŠ¶æ…‹ä¾¡å€¤ ğ‘‰(ğ‘ _ğ‘¡) ã‚’å–å¾—
        # .squeeze() ã«ã‚ˆã£ã¦ [T, 1] â†’ [T] ã«å¤‰å½¢ï¼ˆæå¤±è¨ˆç®—ã®ãŸã‚ï¼‰
        values = self.value_net(obs).squeeze()
        # çŠ¶æ…‹ä¾¡å€¤ã¨å®Ÿéš›ã® Return ã®èª¤å·®ã‚’ MSEï¼ˆå¹³å‡äºŒä¹—èª¤å·®ï¼‰ã§è¨ˆç®—
        value_loss = nn.functional.mse_loss(values, returns)

        # æœ€çµ‚çš„ãªæå¤±ã¯ã€æ–¹ç­–æå¤± + ä¾¡å€¤æå¤±ï¼ˆé‡ã¿ä»˜ãï¼‰
        # return policy_loss + self.value_coef * value_loss

        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ï¼ˆæ¢ç´¢ä¿ƒé€²ï¼‰
        entropy = dist.entropy().mean()
        # entropy_coef = 0.01  # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦èª¿æ•´å¯èƒ½

        # ç·åˆæå¤±
        total_loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy
        return total_loss

    def compute_returns(self, rewards: list[Tensor]) -> list[Tensor]:
        """
        PPOï¼ˆProximal Policy Optimizationï¼‰ã«ãŠã‘ã‚‹ã€Œå‰²å¼•å ±é…¬ï¼ˆReturnï¼‰ã€ã®è¨ˆç®—å‡¦ç†
        ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å…¨ä½“ã®å ±é…¬å±¥æ­´ã‹ã‚‰ã€å„æ™‚ç‚¹ã§ã®ç´¯ç©å ±é…¬ï¼ˆReturnï¼‰ã‚’é€†é †ã§è¨ˆç®—ã€‚

        PPOã§ã¯ã€çŠ¶æ…‹ã®ä¾¡å€¤ï¼ˆvalueï¼‰ã¨å®Ÿéš›ã®Returnã¨ã®å·®åˆ†ï¼ˆAdvantageï¼‰ã‚’ä½¿ã£ã¦å­¦ç¿’
        ğ´_ğ‘¡ = ğº_ğ‘¡ âˆ’ ğ‘‰(ğ‘ _ğ‘¡)
        ãã®ãŸã‚ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã®Return ğº_ğ‘¡ ã‚’æ­£ç¢ºã«è¨ˆç®—ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚‹
        """
        returns = []
        G = 0
        # å ±é…¬ãƒªã‚¹ãƒˆã‚’å¾Œã‚ã‹ã‚‰å‰ã¸å‡¦ç†
        for r in reversed(rewards):
            # ç¾åœ¨ã®å ±é…¬ ğ‘Ÿ ã«ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ç´¯ç©å ±é…¬ ğº ã‚’å‰²å¼•ã—ã¦åŠ ãˆã‚‹
            # ã“ã‚Œã«ã‚ˆã‚Šã€æœªæ¥ã®å ±é…¬ã‚’è€ƒæ…®ã—ãŸç´¯ç©å€¤ãŒå¾—ã‚‰ã‚Œã‚‹
            G = r + self.gamma * G
            # returns ã®å…ˆé ­ã« ğº ã‚’æŒ¿å…¥ã™ã‚‹ã“ã¨ã§ã€å…ƒã®æ™‚é–“é †ã«æˆ»ã™
            returns.insert(0, G)
        return returns

    def get_dim(self) -> tuple[int, int]:
        obs_dim = self.env.observation_space.shape[0]
        act_dim = self.env.action_space.n
        return obs_dim, act_dim

    def get_transaction(self) -> pd.DataFrame:
        # å–å¼•æ˜ç´°ã®è¾æ›¸ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã—ã¦è¿”ã™
        return pd.DataFrame(self.env.trans_man.dict_transaction)

    def infer(self, df: pd.DataFrame, model_path: str):
        """
        éå»ã®ãƒ†ã‚£ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’æ¸ˆãƒ¢ãƒ‡ãƒ«ã§æ¨è«–
        ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ç”¨ã¯åˆ¥é€”ç”¨æ„ã™ã‚‹
        """
        self.env = TrainingEnv(df)
        obs_dim, act_dim = self.get_dim()

        # ---------------------------------------------------------------------
        # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        # ---------------------------------------------------------------------
        checkpoint = torch.load(model_path)

        # è¡Œå‹•åˆ†å¸ƒã‚’å‡ºåŠ›ã™ã‚‹æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.policy_net = PolicyNetwork(obs_dim, act_dim)
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã€ä¿å­˜ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¾©å…ƒ
        self.policy_net.load_state_dict(checkpoint["policy_state_dict"])
        # .eval() ã«ã‚ˆã£ã¦æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆï¼ˆDropout ã‚„ BatchNorm ã‚’ç„¡åŠ¹åŒ–ï¼‰
        self.policy_net.eval()

        # _/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_
        # æ¨è«–ãƒ«ãƒ¼ãƒ—
        # _/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_
        obs, info = self.env.reset()
        done = False
        while not done:
            # çŠ¶æ…‹ã¨ãƒã‚¹ã‚¯ã‚’ãƒ†ãƒ³ã‚½ãƒ«åŒ–ï¼ˆPyTorchãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«æ¸¡ã™ãŸã‚ï¼‰
            obs_tensor = torch.tensor(obs, dtype=torch.float32)
            mask_tensor = torch.tensor(info["action_mask"], dtype=torch.float32)
            # ãƒã‚¹ã‚¯ä»˜ãã§è¡Œå‹•åˆ†å¸ƒã‚’ç”Ÿæˆã—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€log_prob ã¯æ¨è«–ã§ã¯ä½¿ã‚ãªã„ãŒã€ãƒ­ã‚°ã‚„åˆ†æã«æ´»ç”¨å¯èƒ½
            action, log_prob = self.select_action(obs_tensor, mask_tensor)

            """
            é¸æŠã•ã‚ŒãŸè¡Œå‹•ãŒãƒã‚¹ã‚¯ã§ç¦æ­¢ã•ã‚Œã¦ã„ãªã„ã‹ã‚’ç¢ºèª
            ä»Šã®ã¨ã“ã‚ã€é•åè¡Œå‹•ã®æ¤œå‡ºã ã‘ã§ååˆ†ã€‚
            """
            if mask_tensor[action] == 0:
                print(f"âŒ é•åè¡Œå‹•: {action}, Mask: {mask_tensor.tolist()}")

            obs, reward, done, _, info = self.env.step(action)

    def initialize_networks(self, obs_dim: int, act_dim: int):
        """
        ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆæœŸåŒ–
        """
        # è¡Œå‹•åˆ†å¸ƒã‚’å‡ºåŠ›ã™ã‚‹æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.policy_net = PolicyNetwork(obs_dim, act_dim)
        # çŠ¶æ…‹ä¾¡å€¤ã‚’æ¨å®šã™ã‚‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        # ValueNetwork ã¯ å­¦ç¿’æ™‚ã® Advantage è¨ˆç®—å°‚ç”¨
        self.value_net = ValueNetwork(obs_dim)
        # ä¸¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŒæ™‚ã«æ›´æ–°
        self.optimizer = optim.Adam(
            list(self.policy_net.parameters()) + list(self.value_net.parameters()),
            self.lr
        )

    def select_action(self, obs: Tensor, action_mask: Tensor):
        """
        ğŸš¦ è¡Œå‹•é¸æŠé–¢æ•°
        ç¾åœ¨ã®çŠ¶æ…‹ obs ã¨è¡Œå‹•ãƒã‚¹ã‚¯ action_mask ã‚’ä½¿ã£ã¦ã€æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã‚‰è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹
        PPOã§ã¯ã€ç¢ºç‡çš„æ–¹ç­–ã«åŸºã¥ã„ã¦è¡Œå‹•ã‚’é¸ã³ã€ãã®ç¢ºç‡ï¼ˆlog_probï¼‰ã‚‚è¨˜éŒ²ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
        """
        # æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«çŠ¶æ…‹ã¨ãƒã‚¹ã‚¯ã‚’æ¸¡ã—ã¦ã€è¡Œå‹•ã®ãƒ­ã‚¸ãƒƒãƒˆï¼ˆæœªæ­£è¦åŒ–ã‚¹ã‚³ã‚¢ï¼‰ã‚’å–å¾—
        logits = self.policy_net(obs, action_mask)
        # Categorical åˆ†å¸ƒã‚’ä½¿ã£ã¦ã€ãƒ­ã‚¸ãƒƒãƒˆã‹ã‚‰ç¢ºç‡åˆ†å¸ƒã‚’æ§‹ç¯‰
        dist = Categorical(logits=logits)
        # åˆ†å¸ƒ dist ã‹ã‚‰ ç¢ºç‡çš„ã«è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        action = dist.sample()
        # é¸æŠã—ãŸè¡Œå‹•ã®å¯¾æ•°ç¢ºç‡ï¼ˆlog_probï¼‰ã‚’å–å¾—
        log_prob = dist.log_prob(action)
        # action.item() ã«ã‚ˆã£ã¦ã€ãƒ†ãƒ³ã‚½ãƒ«ã‹ã‚‰Pythonã®æ•´æ•°ã«å¤‰æ›
        # log_prob ã¯ãã®ã¾ã¾ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã™ï¼ˆå¾Œã§ loss.backward() ã«ä½¿ã†ãŸã‚ï¼‰
        return action.item(), log_prob

    def train(self, df: pd.DataFrame, model_path: str, num_epochs: int = 3, new_model: bool = False):
        """
        éå»ã®ãƒ†ã‚£ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
        """
        # ç’°å¢ƒã¯å­¦ç¿’ã¨æ¨è«–ã§ç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã€ã“ã“ã§å®šç¾©ã™ã‚‹
        self.env = TrainingEnv(df)
        obs_dim, act_dim = self.get_dim()

        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®åˆæœŸåŒ–
        self.initialize_networks(obs_dim, act_dim)

        # ğŸ” æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Œã°èª­ã¿è¾¼ã‚€ï¼ˆç¶™ç¶šå­¦ç¿’å¯¾å¿œï¼‰
        if not new_model and os.path.exists(model_path):
            checkpoint = torch.load(model_path)
            self.policy_net.load_state_dict(checkpoint["policy_state_dict"])
            self.value_net.load_state_dict(checkpoint["value_state_dict"])
            print(f"ğŸ“¦ æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {model_path}")

        # _/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_
        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        # _/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_
        for epoch in range(num_epochs):
            loss, reward = self.train_one_epoch()
            print(f"Epoch {epoch + 1}: Loss = {loss.item():.4f}, Total Reward = {reward:.3f}")

        # ---------------------------------------------------------------------
        # å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        # ---------------------------------------------------------------------
        # https://docs.pytorch.org/docs/stable/generated/torch.save.html
        obj = {
            "policy_state_dict": self.policy_net.state_dict(),
            "value_state_dict": self.value_net.state_dict()
        }
        torch.save(obj, model_path)
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_path}")

    def train_one_epoch(self) -> tuple[Tensor, float]:
        obs_list, action_list, logprob_list, reward_list, mask_list = [], [], [], [], []
        total_reward = 0.0

        # åˆæœŸçŠ¶æ…‹ã¨ãƒã‚¹ã‚¯å–å¾—
        obs, info = self.env.reset()
        done = False
        while not done:
            # ç’°å¢ƒã‹ã‚‰å¾—ã‚‰ã‚ŒãŸè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ obs ã‚’ PyTorch ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
            obs_tensor = torch.tensor(obs, dtype=torch.float32)
            # è¡Œå‹•ãƒã‚¹ã‚¯ï¼ˆaction_maskï¼‰ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
            mask_tensor = torch.tensor(info["action_mask"], dtype=torch.float32)
            # ãƒã‚¹ã‚¯ä»˜ãã§è¡Œå‹•åˆ†å¸ƒã‚’ç”Ÿæˆã—ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€log_prob ã¯ PPO ã®æå¤±è¨ˆç®—ã«å¿…è¦
            action, log_prob = self.select_action(obs_tensor, mask_tensor)

            """
            1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åˆ†ã®å±¥æ­´ã‚’å¾Œã§ãƒãƒƒãƒåŒ–ã—ã¦ã€PPOã®æå¤±é–¢æ•°ã«æ¸¡ã™
            ã“ã®ã‚ˆã†ã«ã™ã‚‹ã¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å…¨ä½“ã‚’1ã¤ã®ãƒ†ãƒ³ã‚½ãƒ«ãƒãƒƒãƒã¨ã—ã¦æ‰±ãˆã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚
            """
            # ç¾åœ¨ã®è¦³æ¸¬ï¼ˆçŠ¶æ…‹ï¼‰ã‚’ä¿å­˜
            obs_list.append(obs_tensor)
            # é¸æŠã—ãŸè¡Œå‹•ã‚’ä¿å­˜
            action_list.append(torch.tensor(action))
            # é¸æŠã—ãŸè¡Œå‹•ã®å¯¾æ•°ç¢ºç‡ã‚’ä¿å­˜
            logprob_list.append(log_prob)
            # è¡Œå‹•ãƒã‚¹ã‚¯ã‚’ä¿å­˜
            mask_list.append(mask_tensor)
            # çŠ¶æ…‹é·ç§»ã¨å ±é…¬å–å¾—
            obs, reward, done, _, info = self.env.step(action)
            reward_list.append(torch.tensor(reward, dtype=torch.float32))
            total_reward += reward

        # PPO ã«ãŠã‘ã‚‹ã€Œå‰²å¼•å ±é…¬ï¼ˆReturnï¼‰ã€ã®è¨ˆç®—å‡¦ç†
        returns = self.compute_returns(reward_list)

        """
        PPOã®æå¤±è¨ˆç®—ã«å‘ã‘ãŸã€ŒãƒãƒƒãƒåŒ–ã¨å‰å‡¦ç†ã€
        """
        # æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ»ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¥åŠ›
        obs_batch = torch.stack(obs_list)
        # å„ã‚¹ãƒ†ãƒƒãƒ—ã§é¸æŠã—ãŸè¡Œå‹•ï¼ˆæ•´æ•°ï¼‰ã‚’ãƒ†ãƒ³ã‚½ãƒ«åŒ–
        action_batch = torch.stack(action_list)
        # å„è¡Œå‹•ã®å¯¾æ•°ç¢ºç‡ï¼ˆlog_probï¼‰ã‚’ã¾ã¨ã‚ã‚‹
        logprob_batch = torch.stack(logprob_list)
        # å„ã‚¹ãƒ†ãƒƒãƒ—ã®å‰²å¼•å ±é…¬ï¼ˆReturnï¼‰ã‚’ã¾ã¨ã‚ã‚‹
        return_batch = torch.stack(returns)
        # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ« obs_batch ã‚’ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é€šã—ã¦ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã®çŠ¶æ…‹ä¾¡å€¤ ğ‘‰(ğ‘ _ğ‘¡) ã‚’å–å¾—
        value_batch = self.value_net(obs_batch).squeeze(-1)  # æœ€å¾Œã®æ¬¡å…ƒã ã‘ã‚’æ½°ã™
        # Advantageï¼ˆåˆ©å¾—ï¼‰ã®è¨ˆç®—
        adv_batch = return_batch - value_batch.detach()
        # å„ã‚¹ãƒ†ãƒƒãƒ—ã®è¡Œå‹•ãƒã‚¹ã‚¯ã‚’ã¾ã¨ã‚ã‚‹
        mask_batch = torch.stack(mask_list)
        """
        PPOã®æå¤±é–¢æ•°ã‚’è¨ˆç®—ï¼ˆæ–¹ç­–ãƒ»ä¾¡å€¤ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é …ã‚’å«ã‚€ï¼‰
        """
        loss = self.compute_ppo_loss(
            obs_batch,
            action_batch,
            logprob_batch,
            return_batch,
            adv_batch,
            mask_batch
        )
        # å‹¾é…ã‚’ã‚¼ãƒ­ã«åˆæœŸåŒ–
        self.optimizer.zero_grad()
        # æå¤±é–¢æ•°ã‹ã‚‰å‹¾é…ã‚’è¨ˆç®—
        loss.backward()
        # å‹¾é…ã«åŸºã¥ã„ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
        self.optimizer.step()
        return loss, total_reward
