import os
from pathlib import Path
from typing import Callable

from PySide6.QtCore import QObject, Signal
from sb3_contrib import RecurrentPPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv

from funcs.ios import get_excel_sheet
from modules.env import TrainingEnv
from structs.res import AppRes


class SaveBestModelCallback(BaseCallback):
    def __init__(
            self,
            save_path: str,
            reward_path: str,
            should_stop: Callable[[], bool],
            verbose=1
    ):
        super().__init__(verbose)
        self.save_path = save_path
        self.reward_path = reward_path
        self.best_mean_reward = self._load_best_reward()
        self.should_stop = should_stop

    def _load_best_reward(self):
        if os.path.exists(self.reward_path):
            with open(self.reward_path, "r") as f:
                return float(f.read())
        return -float("inf")

    def _save_best_reward(self, reward: float):
        with open(self.reward_path, "w") as f:
            f.write(str(reward))

    def _on_step(self) -> bool:
        if self.should_stop():
            print("ğŸ›‘ Training interrupted by user.")
            return False  # å­¦ç¿’ã‚’ä¸­æ–­

        if "episode" in self.locals["infos"][0]:
            ep_reward = self.locals["infos"][0]["episode"]["r"]
            if ep_reward > self.best_mean_reward:
                self.best_mean_reward = ep_reward
                # â– â– â–  self.model ã¯ã©ã“ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹ï¼Ÿ â– â– â– 
                self.model.save(self.save_path)
                self._save_best_reward(ep_reward)
                if self.verbose > 0:
                    print(f"âœ… New best reward: {ep_reward:.2f} â†’ Model saved.")
        return True


class PPOAgent(QObject):
    finishedTraining = Signal(str)
    finishedInferring = Signal()

    def __init__(self, res: AppRes):
        super().__init__()
        self.res = res
        self._stopping = False
        # self.total_timesteps = 1572864
        # self.total_timesteps = 100000
        self.total_timesteps = 20000

    def get_env(self, file: str, code: str) -> DummyVecEnv:
        # Excel ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ•ãƒ«ãƒ‘ã‚¹ã«
        path_excel = self.get_source_path(file)
        # Excel ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«èª­ã¿è¾¼ã‚€
        df = get_excel_sheet(path_excel, code)

        # ç’°å¢ƒã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆ
        env_raw = TrainingEnv(df)
        env_monitor = Monitor(env_raw, self.res.dir_log)  # Monitorã®åˆ©ç”¨
        env_vec = DummyVecEnv([lambda: env_monitor])

        return env_vec

    def get_model_path(self, code: str) -> tuple[str, str]:
        model_path = os.path.join(self.res.dir_model, f"ppo_{code}.zip")
        reward_path = os.path.join(self.res.dir_model, f"best_reward_{code}.txt")
        return model_path, reward_path

    def get_source_path(self, file: str) -> str:
        path_excel = str(Path(os.path.join(self.res.dir_collection, file)).resolve())
        return path_excel

    def stop(self):
        """å®‰å…¨ã«çµ‚äº†ã•ã›ã‚‹ãŸã‚ã®ãƒ•ãƒ©ã‚°"""
        self._stopping = True

    def train(self, file: str, code: str):
        # å­¦ç¿’ç’°å¢ƒã®å–å¾—
        env = self.get_env(file, code)

        # å­¦ç¿’æ¸ˆãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        model_path, _ = self.get_model_path(code)
        if os.path.exists(model_path):
            print(f"ãƒ¢ãƒ‡ãƒ« {model_path} ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚")
            model = RecurrentPPO.load(model_path, env, verbose=True)
        else:
            print(f"æ–°è¦ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚")
            # PPO ãƒ¢ãƒ‡ãƒ«ã®ç”Ÿæˆ
            # LSTM ã‚’å«ã‚€æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ MlpLstmPolicy ã‚’æŒ‡å®š
            model = RecurrentPPO("MlpLstmPolicy", env, verbose=True)

        # ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
        model.learn(total_timesteps=self.total_timesteps)

        print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ {model_path} ã«ä¿å­˜ã—ã¾ã™ã€‚")
        model.save(model_path)

        # æœ€å¾Œã®å–å¼•å±¥æ­´
        df_transaction = env.envs[0].env.getTransaction()
        print(df_transaction)
        print(f"æç›Š: {df_transaction["æç›Š"].sum():.1f} å††")

        # å­¦ç¿’ç’°å¢ƒã®è§£æ”¾
        env.close()
        self.finishedTraining.emit(file)

    def train_cherry_pick(self, file: str, code: str):
        # å­¦ç¿’ç’°å¢ƒã®å–å¾—
        env = self.get_env(file, code)

        # PPO ãƒ¢ãƒ‡ãƒ«ã®ç”Ÿæˆ
        # LSTM ã‚’å«ã‚€æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ MlpLstmPolicy ã‚’æŒ‡å®š
        model = RecurrentPPO("MlpLstmPolicy", env, verbose=True)

        # ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
        model_path, reward_path = self.get_model_path(code)
        callback = SaveBestModelCallback(
            save_path=model_path,
            reward_path=reward_path,
            should_stop=lambda: self._stopping
        )
        model.learn(total_timesteps=self.total_timesteps, callback=callback)

        # æœ€å¾Œã®å–å¼•å±¥æ­´
        df_transaction = env.envs[0].env.getTransaction()
        print(df_transaction)
        print(f"æç›Š: {df_transaction["æç›Š"].sum():.1f} å††")

        # å­¦ç¿’ç’°å¢ƒã®è§£æ”¾
        env.close()
        self.finishedTraining.emit(file)

    def infer(self, file: str, code: str):
        pass
        """
        # æ¨è«–å°‚ç”¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ä½œæˆã¾ã§ä¿ç•™
        # å­¦ç¿’ç’°å¢ƒã®å–å¾—
        env = self.get_env(file, code)

        # å­¦ç¿’æ¸ˆãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        model_path, reward_path = self.get_model_path(code)
        if os.path.exists(model_path):
            print(f"ãƒ¢ãƒ‡ãƒ« {model_path} ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚")
        else:
            print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ {model_path} ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            self.finishedInferring.emit()
            return
        model = RecurrentPPO.load(model_path, env, verbose=True)

        # å­¦ç¿’ç’°å¢ƒã®ãƒªã‚»ãƒƒãƒˆ
        # obs, _ = env.reset()
        obs = env.reset()
        lstm_state = None
        total_reward = 0.0

        # æ¨è«–ã®å®Ÿè¡Œ
        episode_over = False
        while not episode_over:
            # ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–
            action, lstm_state = model.predict(obs, state=lstm_state, deterministic=True)
            print(action)

            # 1ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
            # obs, reward, terminated, truncated, info = env.step(action)
            obs, rewards, dones, infos = env.step(action)

            # ãƒ¢ãƒ‡ãƒ«å ±é…¬
            total_reward += rewards[0]

            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Œäº†
            # episode_over = terminated or truncated
            episode_over = dones[0]

        # å­¦ç¿’ç’°å¢ƒã®è§£æ”¾
        env.close()

        self.finishedInferring.emit()
        """
