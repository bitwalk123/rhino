import os
from typing import Callable

from stable_baselines3.common.callbacks import BaseCallback


class EpisodeLimitCallback(BaseCallback):
    def __init__(self, max_episodes, verbose=0):
        super().__init__(verbose)
        self.max_episodes = max_episodes
        self.episode_count = 0

    def _on_step(self) -> bool:
        # `done` ãƒ•ãƒ©ã‚°ãŒ True ã®ã¨ãã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†
        if self.locals.get("dones") is not None:
            self.episode_count += sum(self.locals["dones"])
        return self.episode_count < self.max_episodes


class SaveBestModelCallback(BaseCallback):
    def __init__(
            self,
            save_path: str,
            reward_path: str,
            should_stop: Callable[[], bool],
            verbose=1
    ):
        super().__init__(verbose)
        self.save_path = save_path
        self.reward_path = reward_path
        self.best_mean_reward = self._load_best_reward()
        self.should_stop = should_stop

    def _load_best_reward(self):
        if os.path.exists(self.reward_path):
            with open(self.reward_path, "r") as f:
                return float(f.read())
        return -float("inf")

    def _save_best_reward(self, reward: float):
        with open(self.reward_path, "w") as f:
            f.write(str(reward))

    def _on_step(self) -> bool:
        if self.should_stop():
            print("ğŸ›‘ Training interrupted by user.")
            return False  # å­¦ç¿’ã‚’ä¸­æ–­

        if "episode" in self.locals["infos"][0]:
            ep_reward = self.locals["infos"][0]["episode"]["r"]
            if ep_reward > self.best_mean_reward:
                self.best_mean_reward = ep_reward
                # â– â– â–  self.model ã¯ã©ã“ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹ï¼Ÿ â– â– â– 
                self.model.save(self.save_path)
                self._save_best_reward(ep_reward)
                if self.verbose > 0:
                    print(f"âœ… New best reward: {ep_reward:.2f} â†’ Model saved.")
        return True
