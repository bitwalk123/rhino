import os
from typing import Callable

import gymnasium as gym
import numpy as np

from stable_baselines3.common.callbacks import BaseCallback


class ActionMaskWrapper(gym.Wrapper):
    """
    SB3 ã§ç’°å¢ƒã®æ–¹ç­–ãƒã‚¹ã‚¯ã«å¯¾å¿œã•ã›ã‚‹ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼
    """

    def __init__(self, env):
        super().__init__(env)
        self.action_mask = None

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self.action_mask = info.get("action_mask", np.ones(self.env.action_space.n, dtype=np.int8))
        return obs, info

    def step(self, action):
        if self.action_mask[action] == 0:
            # ç„¡åŠ¹ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸ã‚“ã å ´åˆã€å¼·åˆ¶çš„ã« HOLD ã«ç½®ãæ›ãˆã‚‹
            action = 0  # ActionType.HOLD.value
        obs, reward, done, truncated, info = self.env.step(action)
        self.action_mask = info.get("action_mask", np.ones(self.env.action_space.n, dtype=np.int8))
        return obs, reward, done, truncated, info


class EpisodeLimitCallback(BaseCallback):
    """
    SB3 ã§ Episode å›æ•°ã‚’ Epoch ã¨ã—ã¦æ‰±ã†ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """

    def __init__(self, max_episodes, verbose=0):
        super().__init__(verbose)
        self.max_episodes = max_episodes
        self.episode_count = 0

    def _on_step(self) -> bool:
        # `done` ãƒ•ãƒ©ã‚°ãŒ True ã®ã¨ãã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†
        if self.locals.get("dones") is not None:
            self.episode_count += sum(self.locals["dones"])
        return self.episode_count < self.max_episodes


class SaveBestModelCallback(BaseCallback):
    """
    SB3 ã§æœ€é«˜å ±é…¬ã®ãƒ¢ãƒ‡ãƒ«ã®ã¿ä¿å­˜ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """

    def __init__(
            self,
            save_path: str,
            reward_path: str,
            should_stop: Callable[[], bool],
            verbose=1
    ):
        super().__init__(verbose)
        self.save_path = save_path
        self.reward_path = reward_path
        self.best_mean_reward = self._load_best_reward()
        self.should_stop = should_stop

    def _load_best_reward(self):
        if os.path.exists(self.reward_path):
            with open(self.reward_path, "r") as f:
                return float(f.read())
        return -float("inf")

    def _save_best_reward(self, reward: float):
        with open(self.reward_path, "w") as f:
            f.write(str(reward))

    def _on_step(self) -> bool:
        if self.should_stop():
            print("ğŸ›‘ Training interrupted by user.")
            return False  # å­¦ç¿’ã‚’ä¸­æ–­

        if "episode" in self.locals["infos"][0]:
            ep_reward = self.locals["infos"][0]["episode"]["r"]
            if ep_reward > self.best_mean_reward:
                self.best_mean_reward = ep_reward
                # â– â– â–  self.model ã¯ã©ã“ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹ï¼Ÿ â– â– â– 
                self.model.save(self.save_path)
                self._save_best_reward(ep_reward)
                if self.verbose > 0:
                    print(f"âœ… New best reward: {ep_reward:.2f} â†’ Model saved.")
        return True
